{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) 통계적 언어 모델(Statistical Language Model, SLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 통계적 언어 모델은 언어 모델의 전통적인 접근 방법으로 SLM이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <문장에 대한 확률>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 'An adorable little boy is spreading smiles' 가 주어졌을 때, 이 문장에 대한 확률을 구해보자.\\\n",
    " 즉, P(An adorable little boy is spreading smiles)를 구해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장에서 각 단어는 문맥(context)이라는 관계로 인해 이전 단어의 영향을 받아 다음 단어가 나오게 된다.\\\n",
    "그리하여 문장의 확률을 구하기 위해 조건부 확률을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(w1,w2,w3,w4,w5,...wn)=∏n=1nP(wn|w1,...,wn−1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 주어진 문장 'An adorable little boy is spreading smiles'에 적용하면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(An adorable little boy is spreading smiles)=\n",
    "P(An)×P(adorable|An)×P(little|An adorable)×P(boy|An adorable little)×P(is|An adorable little boy) ×P(spreading|An adorable little boy is)×P(smiles|An adorable little boy is spreading) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 다음 단어에 대한 예측 확률을 모두 곱하여 문장에 대한 확률을 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 이제 다음 단어에 대한 예측확률을 구하는 방법에 대해서 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 카운트 기반의 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLM(Statistical Language Model)은 count 기반으로 확률을 계산하는데, 예를들어 P(is|An adorable little boy) 라는 확률\\\n",
    "즉, An adorable little boy라는 sequency다음 나올 단어가 is일 확률을 구하는 방식은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(is|An adorable little boy) =  {count(An adorable little boy is) \\over count(An adorable little boy )} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 언어 모델이 학습한 코퍼스 데이터에서\n",
    "*  count(An adorable little boy) = 100번\n",
    "*  count(An adorable little boy is) = 30번\\\n",
    "위와 같이 count값이 계산되었다 하면, 구하고자 하는 확률은 30%인 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 SLM은 단어의 확률적인 수치를 계산하여 다음 단어를 예측하는 방식을 사용하는데, count기반의 확률 접근방식에는 한계가 있다.\\\n",
    "현실 세계의 언어에서는 'An adorable little boy' 라는 문장 다음에 'is' 가 등장하는 확률이 존재하지만 모델이 학습한 데이터에\\\n",
    "이러한 문장이 존재하지 않을 때의 확률은 0이 되는 모순이 발생한다. 우리의 목표는 현실세계의 확률 분포값을 근사하는 것이 언어 모델을\\\n",
    "사용하는 궁극적인 목표인데, 모델이 현실세계의 언어들을 전부 학습하려면 엄청나게 많은 학습 데이터가 필요할 것이다.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 모델이 현실세계에는 존재하는데 학습을 하지 못하여 확률을 예측해내지 못하는 문제를 희소 문제(sparsity problem)이라고 한다.\\\n",
    "다음 챕터에서는 이러한 문제를 해결하기 위한 n-gram 등 여러가지 일반화 기법을 소개하고 이마저도 한계가 존재해 궁극적으로 인공 신경망 언어 모델까지 소개할 것이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
